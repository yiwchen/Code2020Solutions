{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Test Errors and Cross-Validation\n",
    "\n",
    "<q>Prediction is very difficult, especially if it's about the future.</q>\n",
    "\n",
    "--Nils Bohr, Nobel laureate in Physics \n",
    "\n",
    "This quote reiterates what we've said about making predictions in the regression material. We can make a model that is very good at fitting the data we have but there is no guarantee that model does well on unseen data (the future in time series).\n",
    "\n",
    "Thus, before introducing more time series forecasting models we'll quickly review how we can adjust the way we compute test errors and perform cross validation. These techniques allow us to attempt to find a model that predicts well.\n",
    "\n",
    "## What You'll Accomplish\n",
    "\n",
    "In particular you'll:\n",
    "<ul>\n",
    "    <li>Compute test errors for the average and na&iuml;ve methods on the marathon data,</li>\n",
    "    <li>Learn about the differences for cross-validation,</li>\n",
    "    <li>Perform cross-validation on the measles data to compute the best method among the average, na&iuml;ve and, seasonal na&iuml;ve methods</li>\n",
    "</ul>\n",
    "\n",
    "Let's import the packages we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll use\n",
    "from datetime import datetime\n",
    "\n",
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Errors\n",
    "\n",
    "Although we are working with time series data, the error measures we use will still be MSE and RMSE. \n",
    "\n",
    "In this setting we can refer to the test error as the forecast errors.\n",
    "\n",
    "Let $T$ be the length of the entire time series, $T_{\\text{train}}$ be the length of the training set, let $y_t$ be the target value observed at time $t$, and assume you have some algorithm that produces a prediction, $\\hat{y}_t$ at time $t$.\n",
    "\n",
    "Then the testing MSE is given by:\n",
    "$$\n",
    "MSE = \\frac{1}{T - T_{\\text{train}}} \\sum_{t = T_{\\text{train}} + 1}^T \\left( y_t - \\hat{y}_t \\right)^2,\n",
    "$$\n",
    "\n",
    "and $RMSE = \\sqrt{MSE}$.\n",
    "\n",
    "### Horizons\n",
    "\n",
    "The <i>horizon</i> of your model is how far out you'd like to predict. For example:\n",
    "<ul>\n",
    "    <li>A model that predicts next quarter's earnings has a horizon of $1$ quarter,</li>\n",
    "    <li>Predicting monthly cases of influenza in 2021 would be a model with a horizon of $1$ year or $12$ months,</li>\n",
    "    <li>A model predicting daily mileage in July 2020 would have a horizon of $1$ month or $31$ days.</li>\n",
    "</ul>\n",
    "\n",
    "Your desired forecasting horizon (how far out into the future you'd like to predict) should determine how large your test set is.\n",
    "\n",
    "## Practice\n",
    "\n",
    "Return to the `marathon` data. We'd like to see if we could predict out the winning times over the next $5$ years.\n",
    "\n",
    "Using both the average method and the na&iuml;ve methods compute the test RMSE. Which method performs better on this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "marathon = pd.read_csv(\"marathon.csv\",parse_dates = ['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the train test split here.\n",
    "marathon_copy = marathon.copy()\n",
    "marathon_train = marathon_copy.iloc[:-5,]\n",
    "marathon_test = marathon_copy.drop(marathon_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Average Method Test RMSE is 11.9368\n"
     ]
    }
   ],
   "source": [
    "## Calculate the Average Method RMSE here\n",
    "pred_avg = marathon_train.time.mean()\n",
    "\n",
    "rmse = np.sqrt(np.sum(np.power(marathon_test.time - pred_avg,2))/len(marathon_test))\n",
    "\n",
    "print(\"The Average Method Test RMSE is\", np.round(rmse,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive Method Test RMSE is 7.8855\n"
     ]
    }
   ],
   "source": [
    "## Calculate the Naive Method RMSE here\n",
    "pred_naive = marathon_train.time.iloc[-1]\n",
    "\n",
    "rmse = np.sqrt(np.sum(np.power(marathon_test.time - pred_naive,2))/len(marathon_test))\n",
    "\n",
    "print(\"The Naive Method Test RMSE is\", np.round(rmse,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation with Time Series\n",
    "\n",
    "Just like we can't randomly sample for the train test split, we also can't randomly sample for cross-validation.\n",
    "\n",
    "Let's explain what you can do with a figure.\n",
    "\n",
    "<img src=\"TimeSeriesCV.png\" style=\"width:80%\"></img>\n",
    "\n",
    "Just like we've done up to this point you make a train test split. In the image the training data is blue and the test data is red. In this example our horizon is $4$ time units.\n",
    "\n",
    "Now using the training data we have made three CV splits. We treat each split like a new data set. In the first one we use the first $4$ time points to predict on the second $4$. Then we use the first eight points to predict on the next $4$. Our final split uses the first $12$ points to predict on the last $4$.\n",
    "\n",
    "Just like with regression we'd calculate MSE or RMSE on each CV split and find the arithmetic mean across all splits.\n",
    "\n",
    "If you have a set horizon in mind your cv splits should reflect that horizon. For example, if you use cv to see how well your model predicts $1$ time step forward, but then you actually use that model to predict out $4$ time steps forward you shouldn't be shocked if your predictions don't go too well.\n",
    "\n",
    "### Practice\n",
    "\n",
    "Again return to the pre-1963 measles data. Using cross-validation calculate the average RMSE for the average,  na&iuml;ve, and seasonal na&iuml;ve methods. Use a $24$ month horizon. Which model do we expect to perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "measles = pd.read_csv(\"measles.csv\", parse_dates=['month'])\n",
    "measles_pre1963 = measles.loc[measles.month < datetime(1963,1,1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the train test split here\n",
    "measles_copy = measles_pre1963.copy()\n",
    "measles_train = measles_copy.loc[measles_copy.month < datetime(1961,1,1),]\n",
    "measle_test = measles_copy.drop(measles_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make CV splits here\n",
    "test_index = []\n",
    "train_index = []\n",
    "\n",
    "for i in range(1,int(len(measles_train)/24)):\n",
    "    if i > 1:\n",
    "        test = measles_train.index[-24*i:-24*(i-1)]\n",
    "    else:\n",
    "        test = measles_train.index[-24:]\n",
    "        \n",
    "    train = measles_train.index[:-24*i]\n",
    "    \n",
    "    test_index.append(test)\n",
    "    train_index.append(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make any functions you want to calculate\n",
    "## average rmse here\n",
    "def get_rmse(pred,actual):\n",
    "    res = pred - actual\n",
    "    return np.sqrt(np.sum(np.power(res,2))/len(res))\n",
    "\n",
    "\n",
    "def naive_forecast(train,test):\n",
    "    return list(train.cases)[-1]*np.ones(len(test))\n",
    "\n",
    "def avg_forecast(train,test):\n",
    "    return train.cases.mean()*np.ones(len(test))\n",
    "\n",
    "def naive_seasonal_forecast(train,test):\n",
    "    return list(train.cases)[-len(test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE for Average Method Here \n",
    "\n",
    "avg_rmses = []\n",
    "\n",
    "for i,j in zip(train_index,test_index):\n",
    "    pred = avg_forecast(measles_train.iloc[i,],measles_train.iloc[j,])\n",
    "    avg_rmses.append(get_rmse(pred,measles_train.iloc[j,].cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE for the Naive Method here\n",
    "\n",
    "naive_rmses = []\n",
    "\n",
    "for i,j in zip(train_index,test_index):\n",
    "    pred = naive_forecast(measles_train.iloc[i,],measles_train.iloc[j,])\n",
    "    naive_rmses.append(get_rmse(pred,measles_train.iloc[j,].cases))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[598.9642448983634,\n",
       " 1274.239446362679,\n",
       " 2265.310758667193,\n",
       " 1058.3328871390136,\n",
       " 1637.3572329621088,\n",
       " 852.3974034060248,\n",
       " 758.1745896120058,\n",
       " 2184.4131458281117,\n",
       " 7240.845415649566,\n",
       " 7687.755968638269,\n",
       " 3016.347252887174,\n",
       " 1603.931614814048,\n",
       " 3208.0060201003366,\n",
       " 1442.7921425717104,\n",
       " 2681.3602813373163]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RMSE for the Seasonal Naive method here\n",
    "\n",
    "\n",
    "seasonal_naive_rmses = []\n",
    "\n",
    "for i,j in zip(train_index,test_index):\n",
    "    pred = naive_seasonal_forecast(measles_train.iloc[i,],measles_train.iloc[j,])\n",
    "    seasonal_naive_rmses.append(get_rmse(pred,measles_train.iloc[j,].cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Method 2590.5528\n",
      "Naive Method 2857.4306\n",
      "Seasonal Naive Method 2500.6819\n"
     ]
    }
   ],
   "source": [
    "## Compare here\n",
    "print(\"Average Method\",np.round(np.mean(avg_rmses),4))\n",
    "print(\"Naive Method\",np.round(np.mean(naive_rmses),4))\n",
    "print(\"Seasonal Naive Method\",np.round(np.mean(seasonal_naive_rmses),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this notebook!\n",
    "\n",
    "Next we'll see one of the two more advanced forecasting techniques touched on in the Time Series notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Section 3.4 of <a href=\"https://otexts.com/fpp2/\">Forecasting: Principles and Practice</a>, by Rob J Hyndman and George Athanasopoulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
